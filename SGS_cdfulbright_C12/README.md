# ğŸ§  Jailbreak Prompt Classifier â€“ Model Evaluation & Improvement Report

---

## ğŸ‡¬ğŸ‡§ English Version

### 1. Model Overview
This project builds a binary text classification model to detect **jailbreak prompts** â€” malicious or rule-bypassing instructions given to LLMs (e.g., â€œpretend to be DANâ€, â€œignore safety rulesâ€).  
The model combines:

- **TF-IDF (word uni/bi-grams)** features  
- **Lightweight meta-features** such as:
  - Text length, word count
  - Number of â€œ!â€ and â€œ?â€
  - Uppercase character count
  - Presence of URLs
  - Jailbreak-related keywords (`pretend`, `ignore`, `bypass`, etc.)
- **LightGBM** as the main classifier

This architecture is simple, interpretable, and computationally efficient â€” ideal for a hackathon setup.

---

### 2. Performance Summary

| Metric | Score |
|:-------|:------|
| OOF AUC (CV) | ~0.960 |
| Public Leaderboard | **0.94476** |
| Private Leaderboard | **0.97295** |

âœ… The **Private AUC is higher** than both Public and OOF, indicating **strong generalization** and no overfitting to the training or public test data.

---

### 3. Strengths
- Balanced mix of **lexical** (TF-IDF) and **behavioral** (meta) features.  
- Clean text preprocessing that retains key jailbreak signals.  
- Stable AUC across folds and test datasets.  
- Ensemble setup (LogReg, NB-SVM, Calibrated SVC, LightGBM) further enhances robustness.

---

### 4. Limitations
- Only **3-fold CV** â†’ higher variance and less stable validation.  
- `stop_words="english"` may remove meaningful negations like â€œnotâ€, â€œwithoutâ€.  
- Missing **char-level TF-IDF** â€” important for obfuscated variants (â€œja1lbr3akâ€, â€œbyp@ssâ€).  
- LightGBM uses fixed `n_estimators=400` instead of `best_iteration_`.  
- Lack of **regularization parameters** (`min_child_samples`, `reg_alpha`, etc.).

---

### 5. Recommended Improvements
1. **Cross-validation:**  
   - Increase to **5-fold** Stratified CV.  
   - Stratify by both label and text length to balance distributions.
2. **Feature Engineering:**  
   - Add **char-level TF-IDF (3â€“6 n-grams)** and blend with word TF-IDF.  
   - Try a version **without stopwords** for comparison.
3. **Model Regularization (LightGBM):**  
   - `min_child_samples=50â€“200`  
   - `reg_alpha=1â€“5`, `reg_lambda=1â€“5`  
   - `feature_fraction=0.6â€“0.9`, `bagging_fraction=0.7â€“0.9`
4. **Training Strategy:**  
   - Use **average best_iteration** from folds for final full-data training.  
   - Try **3â€“5 random seeds** and average predictions for stability.

---

### 6. Overall Assessment
This model is a **strong and well-generalized baseline**, achieving high accuracy and AUC (0.96â€“0.97).  
Future work should focus on:
- Adding char-level features  
- Stronger regularization  
- More stable CV setup  
â†’ to maintain consistency across unseen datasets.

---

## ğŸ‡»ğŸ‡³ PhiÃªn báº£n Tiáº¿ng Viá»‡t

### 1. Tá»•ng quan mÃ´ hÃ¬nh
Dá»± Ã¡n xÃ¢y dá»±ng mÃ´ hÃ¬nh phÃ¢n loáº¡i nhá»‹ phÃ¢n nháº±m phÃ¡t hiá»‡n **jailbreak prompt** â€“ cÃ¡c Ä‘oáº¡n vÄƒn báº£n cá»‘ tÃ¬nh â€œlÃ¡ch luáº­tâ€ hoáº·c vÆ°á»£t qua giá»›i háº¡n cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ (vÃ­ dá»¥: â€œpretend to be DANâ€, â€œignore all rulesâ€).  
MÃ´ hÃ¬nh káº¿t há»£p:

- **TF-IDF (tá»« Ä‘Æ¡n + cá»¥m 2 tá»«)**  
- **CÃ¡c Ä‘áº·c trÆ°ng bá»• sung**:
  - Äá»™ dÃ i vÄƒn báº£n, sá»‘ lÆ°á»£ng tá»«
  - Sá»‘ dáº¥u â€œ!â€ vÃ  â€œ?â€
  - Sá»‘ kÃ½ tá»± viáº¿t hoa
  - CÃ³ chá»©a â€œURLâ€ hay khÃ´ng
  - Tá»« khÃ³a liÃªn quan jailbreak (`pretend`, `ignore`, `bypass`, â€¦)
- **LightGBM** lÃ m mÃ´ hÃ¬nh chÃ­nh

CÃ¡ch tiáº¿p cáº­n Ä‘Æ¡n giáº£n, hiá»‡u quáº£ vÃ  phÃ¹ há»£p vá»›i mÃ´i trÆ°á»ng hackathon.

---

### 2. Káº¿t quáº£ tá»•ng há»£p

| Chá»‰ sá»‘ | GiÃ¡ trá»‹ |
|:-------|:--------|
| AUC Cross-validation | ~0.960 |
| Public Leaderboard | **0.94476** |
| Private Leaderboard | **0.97295** |

âœ… Äiá»ƒm **Private cao hÆ¡n** Public vÃ  CV â†’ mÃ´ hÃ¬nh **tá»•ng quÃ¡t tá»‘t**, khÃ´ng bá»‹ overfit theo train hoáº·c public test.

---

### 3. Äiá»ƒm máº¡nh
- Káº¿t há»£p há»£p lÃ½ giá»¯a Ä‘áº·c trÆ°ng ngÃ´n ngá»¯ vÃ  hÃ nh vi.  
- Tiá»n xá»­ lÃ½ dá»¯ liá»‡u giá»¯ láº¡i tÃ­n hiá»‡u jailbreak quan trá»ng.  
- AUC á»•n Ä‘á»‹nh giá»¯a cÃ¡c fold vÃ  cÃ¡c bá»™ test.  
- CÃ³ sá»­ dá»¥ng **ensemble nhiá»u mÃ´ hÃ¬nh** (LogReg, NB-SVM, SVC, LGBM) giÃºp tÄƒng Ä‘á»™ bá»n vá»¯ng.

---

### 4. Háº¡n cháº¿
- Chá»‰ dÃ¹ng **3-fold CV**, nÃªn káº¿t quáº£ dao Ä‘á»™ng nháº¹.  
- DÃ¹ng stopwords tiáº¿ng Anh cÃ³ thá»ƒ xÃ³a máº¥t cÃ¡c tá»« phá»§ Ä‘á»‹nh quan trá»ng.  
- Thiáº¿u **TF-IDF kÃ½ tá»± (char-level)** Ä‘á»ƒ phÃ¡t hiá»‡n biáº¿n thá»ƒ kiá»ƒu â€œbyp@ssâ€, â€œja1lbr3akâ€.  
- LightGBM chÆ°a dÃ¹ng `best_iteration_`, dáº«n Ä‘áº¿n train chÆ°a tá»‘i Æ°u.  
- Thiáº¿u tham sá»‘ regularization Ä‘á»ƒ á»•n Ä‘á»‹nh mÃ´ hÃ¬nh.

---

### 5. HÆ°á»›ng cáº£i tiáº¿n Ä‘á» xuáº¥t
1. **Cross-validation:**  
   - DÃ¹ng **5-fold StratifiedKFold**, stratify theo cáº£ nhÃ£n vÃ  Ä‘á»™ dÃ i vÄƒn báº£n.  
2. **Ká»¹ thuáº­t Ä‘áº·c trÆ°ng:**  
   - ThÃªm **TF-IDF kÃ½ tá»± (3â€“6 kÃ½ tá»±)** vÃ  gá»™p vá»›i TF-IDF tá»«.  
   - So sÃ¡nh báº£n cÃ³/khÃ´ng stopwords Ä‘á»ƒ tÃ¬m cáº¥u hÃ¬nh tá»‘i Æ°u.  
3. **Äiá»u chá»‰nh LightGBM:**  
   - `min_child_samples=50â€“200`  
   - `reg_alpha=1â€“5`, `reg_lambda=1â€“5`  
   - `feature_fraction=0.6â€“0.9`, `bagging_fraction=0.7â€“0.9`
4. **Chiáº¿n lÆ°á»£c huáº¥n luyá»‡n:**  
   - DÃ¹ng trung bÃ¬nh `best_iteration` tá»« cÃ¡c fold khi train full.  
   - Train thÃªm vá»›i **3â€“5 seed khÃ¡c nhau** rá»“i trung bÃ¬nh káº¿t quáº£ Ä‘á»ƒ tÄƒng Ä‘á»™ á»•n Ä‘á»‹nh.

---

### 6. ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ
MÃ´ hÃ¬nh hiá»‡n táº¡i Ä‘áº¡t **AUC cao (0.96â€“0.97)** vÃ  thá»ƒ hiá»‡n kháº£ nÄƒng **tá»•ng quÃ¡t tá»‘t**.  
Giai Ä‘oáº¡n tiáº¿p theo nÃªn táº­p trung tinh chá»‰nh nhá»:
- TF-IDF kÃ½ tá»±  
- Regularization  
- Cross-validation á»•n Ä‘á»‹nh hÆ¡n  

â†’ Ä‘á»ƒ giá»¯ vá»¯ng hiá»‡u suáº¥t trÃªn dá»¯ liá»‡u test áº©n vÃ  Ä‘áº£m báº£o Ä‘iá»ƒm Private Leaderboard cao bá»n vá»¯ng.

---

**Author:** Ngoc - cdfulbright 
**Competition:** RMIT Hackathon 2025 â€“ Jailbreak Detection Challenge  
**Last Updated:** October 2025
